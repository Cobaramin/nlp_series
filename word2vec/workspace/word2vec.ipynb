{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "- Continuous bag of words\n",
    "- skip gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'ฉัน กิน ข้าว มัน ไก่',\n",
    "    'ข้าว มัน ไก่ อร่อย ดี นะ',\n",
    "    'ฉัน ชอบ ผัด ไทย',\n",
    "    'ไก่ ย่าง วิเชียร์บุรี',\n",
    "    'วันนี้ จะ กิน ข้าว หมูทอด มัน อร่อย มาก',\n",
    "    'หิว ข้าว จัง',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize corpus & create a list of unique word\n",
    "- word2idx\n",
    "- idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ฉัน': 0,\n",
       " 'กิน': 1,\n",
       " 'ข้าว': 2,\n",
       " 'มัน': 3,\n",
       " 'ไก่': 4,\n",
       " 'อร่อย': 5,\n",
       " 'ดี': 6,\n",
       " 'นะ': 7,\n",
       " 'ชอบ': 8,\n",
       " 'ผัด': 9,\n",
       " 'ไทย': 10,\n",
       " 'ย่าง': 11,\n",
       " 'วิเชียร์บุรี': 12,\n",
       " 'วันนี้': 13,\n",
       " 'จะ': 14,\n",
       " 'หมูทอด': 15,\n",
       " 'มาก': 16,\n",
       " 'หิว': 17,\n",
       " 'จัง': 18}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pair center word & contex word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of pairs center word & contex word : (102, 2)\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "idx_pairs = []\n",
    "for sentence in tokenized_corpus: #sentence\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    for center_word_pos in range(len(indices)): # center word pos\n",
    "        for w in range(-window_size, window_size + 1):  # context word pos\n",
    "            context_word_pos = center_word_pos + w\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            idx_pairs.append((\n",
    "                indices[center_word_pos], \n",
    "                indices[context_word_pos]\n",
    "            ))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "print('shape of pairs center word & contex word :', idx_pairs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example of idx_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 2],\n",
       "       [0, 3],\n",
       "       [1, 0],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_pairs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example of word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ฉัน', 'กิน'],\n",
       "       ['ฉัน', 'ข้าว'],\n",
       "       ['ฉัน', 'มัน'],\n",
       "       ['กิน', 'ฉัน'],\n",
       "       ['กิน', 'ข้าว']], dtype='<U4')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vectorize(idx2word.get)(idx_pairs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram objective\n",
    "Predict context given center word\n",
    "> $$\\Pr(context \\mid center\\;;\\;\\theta)$$\n",
    "Example :\n",
    "- ฉัน กิน ข้าว มัน ไก่\n",
    "- ฉัน กิน ... มัน ไก่\n",
    "\n",
    "Ojective function : \n",
    "> $$\\max\\prod_{center}\\prod_{context}\\Pr(context \\mid center\\;;\\;\\theta)$$\n",
    "> 1. Applying log give better computational \n",
    "> $$\\min_{\\theta}-\\log\\prod_{center}\\prod_{context}\\Pr(context \\mid center\\;;\\;\\theta)$$\n",
    "> 2. Replace Produce with sum\n",
    "> $$\\log(a.b) = \\log a + \\log b$$\n",
    "> 3. dividing by number of paragraph (T)\n",
    "> $$loss = -\\frac{1}{T}\\sum_{center}\\sum_{context}\\log\\Pr(context \\mid center\\;;\\;\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pr\n",
    "> $$\\Pr(context \\mid center) = \\frac{\\exp(u_{context}^{T}v_{center})}{\\sum_{w\\in vocab}\\exp(u_{w}^{T}v_{center})}$$\n",
    "> - **Pr** is softmax function\n",
    "> - **u** is vector of context\n",
    "> - **v** is vector of center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture\n",
    "![alt text](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png \"Network Archintecture\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 5.578896999359131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 500: 2.050281286239624\n",
      "Loss at epo 1000: 1.83573317527771\n",
      "Loss at epo 1500: 1.7598172426223755\n",
      "Loss at epo 2000: 1.728287935256958\n",
      "Loss at epo 2500: 1.7139993906021118\n",
      "Loss at epo 3000: 1.7066351175308228\n",
      "Loss at epo 3500: 1.7023707628250122\n",
      "Loss at epo 4000: 1.699673056602478\n",
      "Loss at epo 4500: 1.697850227355957\n",
      "Loss at epo 5000: 1.6965572834014893\n"
     ]
    }
   ],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "embedding_dims = 10\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 5001\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.data[0]\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 500 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 dimension:  [10, 19]\n",
      "W2 dimension:  [19, 10]\n"
     ]
    }
   ],
   "source": [
    "print('W1 dimension: ', list(W1.shape))\n",
    "print('W2 dimension: ', list(W2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5298,  0.6374,  0.3966,  0.5490, -0.6637,  1.3452, -0.6883,\n",
      "         0.0556, -0.9485, -2.1520])\n"
     ]
    }
   ],
   "source": [
    "vec1 = W1[:,word2idx['กิน']]\n",
    "print(vec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4906, -0.2285,  2.5428,  0.4268,  0.3793,  1.0411, -0.1335,\n",
      "        -1.0654, -0.4700,  1.5466])\n"
     ]
    }
   ],
   "source": [
    "vec2 = W2[word2idx['กิน'],:]\n",
    "print(vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using avg of W1 and W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5196,  0.2045,  1.4697,  0.4879, -0.1422,  1.1931, -0.4109,\n",
      "        -0.5049, -0.7093, -0.3027])\n"
     ]
    }
   ],
   "source": [
    "print((vec1+vec2)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous bag of words using lib: gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ฉัน', 'กิน', 'ข้าว', 'มัน', 'ไก่'],\n",
       " ['ข้าว', 'มัน', 'ไก่', 'อร่อย', 'ดี', 'นะ'],\n",
       " ['ฉัน', 'ชอบ', 'ผัด', 'ไทย'],\n",
       " ['ไก่', 'ย่าง', 'วิเชียร์บุรี'],\n",
       " ['วันนี้', 'จะ', 'กิน', 'ข้าว', 'หมูทอด', 'มัน', 'อร่อย', 'มาก'],\n",
       " ['หิว', 'ข้าว', 'จัง']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "\n",
    "model = Word2Vec(tokenized_corpus, size=10, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ฉัน', 'กิน', 'ข้าว', 'มัน', 'ไก่', 'อร่อย', 'ดี', 'นะ', 'ชอบ', 'ผัด', 'ไทย', 'ย่าง', 'วิเชียร์บุรี', 'วันนี้', 'จะ', 'หมูทอด', 'มาก', 'หิว', 'จัง']\n"
     ]
    }
   ],
   "source": [
    "print(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01344029,  0.02174879,  0.03583036, -0.00045144, -0.04118928,\n",
       "        0.02446591, -0.01483102,  0.03524507, -0.01538611, -0.01963457],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['กิน']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
